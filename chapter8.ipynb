{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[##############################] | ETA: 00:00:00\n",
      "Total time elapsed: 00:07:20\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyprind\n",
    "import os\n",
    "pbar = pyprind.ProgBar(50000)\n",
    "labels={\"pos\":1, \"neg\":0}\n",
    "df = pd.DataFrame()\n",
    "for s in (\"test\", \"train\"):\n",
    "    for l in (\"pos\", \"neg\"):\n",
    "        path = \"aclImdb/%s/%s\" %(s, l)\n",
    "        for file in os.listdir(path):\n",
    "            with open(os.path.join(path, file)) as infile:\n",
    "                txt = infile.read()\n",
    "            df = df.append([[txt, labels[l]]], ignore_index=True)\n",
    "            pbar.update()\n",
    "df.columns=[\"review\", \"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I really love this movie!!! I haven't played F...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I don't know what some people were thinking wh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Moving beyond words is this heart breaking sto...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is an excellent Anderson production worth...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I thought the movie was good, but I like to re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  I really love this movie!!! I haven't played F...          1\n",
       "1  I don't know what some people were thinking wh...          1\n",
       "2  Moving beyond words is this heart breaking sto...          1\n",
       "3  This is an excellent Anderson production worth...          1\n",
       "4  I thought the movie was good, but I like to re...          1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "df.to_csv(\"movie_data.csv\", index=False)\n",
    "df = pd.read_csv(\"movie_data.csv\")\n",
    "df.columns=[\"review\", \"sentiment\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I really love this movie!!! I haven't played F...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I don't know what some people were thinking wh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Moving beyond words is this heart breaking sto...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is an excellent Anderson production worth...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I thought the movie was good, but I like to re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  I really love this movie!!! I haven't played F...          1\n",
       "1  I don't know what some people were thinking wh...          1\n",
       "2  Moving beyond words is this heart breaking sto...          1\n",
       "3  This is an excellent Anderson production worth...          1\n",
       "4  I thought the movie was good, but I like to re...          1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"movie_data.csv\")\n",
    "df.columns=[\"review\", \"sentiment\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf - idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 0, 'is': 1, 'sun': 3, 'weather': 6, 'the': 5, 'sweet': 4, 'shining': 2}\n",
      "[[0 1 1 1 0 1 0]\n",
      " [0 1 0 0 1 1 1]\n",
      " [1 2 1 1 1 2 1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count = CountVectorizer()\n",
    "docs = np.array([\n",
    "'The sun is shining',\n",
    "'The weather is sweet',\n",
    "'The sun is shining and the weather is sweet'\n",
    "    ])\n",
    "bag = count.fit_transform(docs)\n",
    "print(count.vocabulary_)\n",
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* tf(t, d) = so lan suat hien token t trong van ban d\n",
    "* df(d, t) = so document d co chua t\n",
    "* $idf(t,d) = \\log\\frac{n_d}{1 + df(d,t)}$\n",
    "* $tf-idf = tf(t,d) * idf(t,d)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.    0.43  0.56  0.56  0.    0.43  0.  ]\n",
      " [ 0.    0.43  0.    0.    0.56  0.43  0.56]\n",
      " [ 0.4   0.48  0.31  0.31  0.31  0.48  0.31]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer()\n",
    "np.set_printoptions(precision=2)\n",
    "print(tfidf.fit_transform(count.fit_transform(docs)).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### in sklearn:\n",
    "* $idf(t,d) = \\log\\frac{1 + n_d}{1 + df(d,t}$\n",
    "* $tf-idf(t,d) = tf(t,d) * (idf(t,d)+1 ) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cleaning text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocessor(text):\n",
    "    text = re.sub(r\"<[^>]>\", '', text)\n",
    "    emoticons = re.findall(\"(?:|;|=)(?:-)?(?:\\)\\(|D|P)\", text)\n",
    "    text = re.sub(\"[\\W]+\", \" \", text.lower()) + \\\n",
    "            \" \".join(emoticons).replace('-', '')\n",
    "    return text\n",
    "df[\"review\"] = df[\"review\"].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing documents into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'and', 'thu', 'they', 'run']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "tokenizer_porter('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'thu', 'run']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words(\"english\")\n",
    "[w for w in tokenizer_porter(\"a runners like running and thus they run\")\n",
    "    if w not in stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a logistic regression model for document classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = df.loc[:25000, \"review\"].values\n",
    "y_train = df.loc[:25000, \"sentiment\"].values\n",
    "X_test = df.loc[25000:, \"review\"].values\n",
    "y_test = df.loc[25000:, \"sentiment\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeovach/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/jeovach/anaconda3/lib/python3.5/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "ename": "JoblibValueError",
     "evalue": "JoblibValueError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\n/home/jeovach/anaconda3/lib/python3.5/runpy.py in _run_module_as_main(mod_name='ipykernel.__main__', alter_argv=1)\n    179         sys.exit(msg)\n    180     main_globals = sys.modules[\"__main__\"].__dict__\n    181     if alter_argv:\n    182         sys.argv[0] = mod_spec.origin\n    183     return _run_code(code, main_globals, None,\n--> 184                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel.__main__', loader=<_f...b/python3.5/site-packages/ipykernel/__main__.py')\n    185 \n    186 def run_module(mod_name, init_globals=None,\n    187                run_name=None, alter_sys=False):\n    188     \"\"\"Execute a module's code without importing it\n\n...........................................................................\n/home/jeovach/anaconda3/lib/python3.5/runpy.py in _run_code(code=<code object <module> at 0x7fd2f2be24b0, file \"/...3.5/site-packages/ipykernel/__main__.py\", line 1>, run_globals={'__builtins__': <module 'builtins' (built-in)>, '__cached__': '/home/jeovach/.local/lib/python3.5/site-packages/ipykernel/__pycache__/__main__.cpython-35.pyc', '__doc__': None, '__file__': '/home/jeovach/.local/lib/python3.5/site-packages/ipykernel/__main__.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': 'ipykernel', '__spec__': ModuleSpec(name='ipykernel.__main__', loader=<_f...b/python3.5/site-packages/ipykernel/__main__.py'), 'app': <module 'ipykernel.kernelapp' from '/home/jeovac.../python3.5/site-packages/ipykernel/kernelapp.py'>}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel.__main__', loader=<_f...b/python3.5/site-packages/ipykernel/__main__.py'), pkg_name='ipykernel', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x7fd2f2be24b0, file \"/...3.5/site-packages/ipykernel/__main__.py\", line 1>\n        run_globals = {'__builtins__': <module 'builtins' (built-in)>, '__cached__': '/home/jeovach/.local/lib/python3.5/site-packages/ipykernel/__pycache__/__main__.cpython-35.pyc', '__doc__': None, '__file__': '/home/jeovach/.local/lib/python3.5/site-packages/ipykernel/__main__.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': 'ipykernel', '__spec__': ModuleSpec(name='ipykernel.__main__', loader=<_f...b/python3.5/site-packages/ipykernel/__main__.py'), 'app': <module 'ipykernel.kernelapp' from '/home/jeovac.../python3.5/site-packages/ipykernel/kernelapp.py'>}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\n/home/jeovach/.local/lib/python3.5/site-packages/ipykernel/__main__.py in <module>()\n      1 \n      2 \n----> 3 \n      4 if __name__ == '__main__':\n      5     from ipykernel import kernelapp as app\n      6     app.launch_new_instance()\n      7 \n      8 \n      9 \n     10 \n\n...........................................................................\n/home/jeovach/.local/lib/python3.5/site-packages/traitlets/config/application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\n/home/jeovach/.local/lib/python3.5/site-packages/ipykernel/kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    469             return self.subapp.start()\n    470         if self.poller is not None:\n    471             self.poller.start()\n    472         self.kernel.start()\n    473         try:\n--> 474             ioloop.IOLoop.instance().start()\n    475         except KeyboardInterrupt:\n    476             pass\n    477 \n    478 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\n/home/jeovach/.local/lib/python3.5/site-packages/zmq/eventloop/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    172             )\n    173         return loop\n    174     \n    175     def start(self):\n    176         try:\n--> 177             super(ZMQIOLoop, self).start()\n        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    178         except ZMQError as e:\n    179             if e.errno == ETERM:\n    180                 # quietly return on ETERM\n    181                 pass\n\n...........................................................................\n/home/jeovach/.local/lib/python3.5/site-packages/tornado/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    882                 self._events.update(event_pairs)\n    883                 while self._events:\n    884                     fd, events = self._events.popitem()\n    885                     try:\n    886                         fd_obj, handler_func = self._handlers[fd]\n--> 887                         handler_func(fd_obj, events)\n        handler_func = <function wrap.<locals>.null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 1\n    888                     except (OSError, IOError) as e:\n    889                         if errno_from_exception(e) == errno.EPIPE:\n    890                             # Happens when the client closes the connection\n    891                             pass\n\n...........................................................................\n/home/jeovach/.local/lib/python3.5/site-packages/tornado/stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    270         # Fast path when there are no active contexts.\n    271         def null_wrapper(*args, **kwargs):\n    272             try:\n    273                 current_state = _state.contexts\n    274                 _state.contexts = cap_contexts[0]\n--> 275                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    276             finally:\n    277                 _state.contexts = current_state\n    278         null_wrapper._wrapped = True\n    279         return null_wrapper\n\n...........................................................................\n/home/jeovach/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    435             # dispatch events:\n    436             if events & IOLoop.ERROR:\n    437                 gen_log.error(\"got POLLERR event on ZMQStream, which doesn't make sense\")\n    438                 return\n    439             if events & IOLoop.READ:\n--> 440                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    441                 if not self.socket:\n    442                     return\n    443             if events & IOLoop.WRITE:\n    444                 self._handle_send()\n\n...........................................................................\n/home/jeovach/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    467                 gen_log.error(\"RECV Error: %s\"%zmq.strerror(e.errno))\n    468         else:\n    469             if self._recv_callback:\n    470                 callback = self._recv_callback\n    471                 # self._recv_callback = None\n--> 472                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    473                 \n    474         # self.update_state()\n    475         \n    476 \n\n...........................................................................\n/home/jeovach/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    409         close our socket.\"\"\"\n    410         try:\n    411             # Use a NullContext to ensure that all StackContexts are run\n    412             # inside our blanket exception handler rather than outside.\n    413             with stack_context.NullContext():\n--> 414                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    415         except:\n    416             gen_log.error(\"Uncaught exception, closing connection.\",\n    417                           exc_info=True)\n    418             # Close the socket on an uncaught exception from a user callback\n\n...........................................................................\n/home/jeovach/.local/lib/python3.5/site-packages/tornado/stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    270         # Fast path when there are no active contexts.\n    271         def null_wrapper(*args, **kwargs):\n    272             try:\n    273                 current_state = _state.contexts\n    274                 _state.contexts = cap_contexts[0]\n--> 275                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    276             finally:\n    277                 _state.contexts = current_state\n    278         null_wrapper._wrapped = True\n    279         return null_wrapper\n\n...........................................................................\n/home/jeovach/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    271         if self.control_stream:\n    272             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    273 \n    274         def make_dispatcher(stream):\n    275             def dispatcher(msg):\n--> 276                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    277             return dispatcher\n    278 \n    279         for s in self.shell_streams:\n    280             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\n/home/jeovach/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': 'from sklearn.grid_search import GridSearchCV\\nfro...     n_jobs=-1)\\ngs_lr_tfidf.fit(X_train, y_train)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': '2017-03-01T22:50:28.287994', 'msg_id': '4EE484EE23394C359C3E459091E4216B', 'msg_type': 'execute_request', 'session': '9C093F067364436688AACCCE8EB0BEA3', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '4EE484EE23394C359C3E459091E4216B', 'msg_type': 'execute_request', 'parent_header': {}})\n    223             self.log.error(\"UNKNOWN MESSAGE TYPE: %r\", msg_type)\n    224         else:\n    225             self.log.debug(\"%s: %s\", msg_type, msg)\n    226             self.pre_handler_hook()\n    227             try:\n--> 228                 handler(stream, idents, msg)\n        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'9C093F067364436688AACCCE8EB0BEA3']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': 'from sklearn.grid_search import GridSearchCV\\nfro...     n_jobs=-1)\\ngs_lr_tfidf.fit(X_train, y_train)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': '2017-03-01T22:50:28.287994', 'msg_id': '4EE484EE23394C359C3E459091E4216B', 'msg_type': 'execute_request', 'session': '9C093F067364436688AACCCE8EB0BEA3', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '4EE484EE23394C359C3E459091E4216B', 'msg_type': 'execute_request', 'parent_header': {}}\n    229             except Exception:\n    230                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    231             finally:\n    232                 self.post_handler_hook()\n\n...........................................................................\n/home/jeovach/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'9C093F067364436688AACCCE8EB0BEA3'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': 'from sklearn.grid_search import GridSearchCV\\nfro...     n_jobs=-1)\\ngs_lr_tfidf.fit(X_train, y_train)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': '2017-03-01T22:50:28.287994', 'msg_id': '4EE484EE23394C359C3E459091E4216B', 'msg_type': 'execute_request', 'session': '9C093F067364436688AACCCE8EB0BEA3', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '4EE484EE23394C359C3E459091E4216B', 'msg_type': 'execute_request', 'parent_header': {}})\n    385         if not silent:\n    386             self.execution_count += 1\n    387             self._publish_execute_input(code, parent, self.execution_count)\n    388 \n    389         reply_content = self.do_execute(code, silent, store_history,\n--> 390                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    391 \n    392         # Flush output before sending the reply.\n    393         sys.stdout.flush()\n    394         sys.stderr.flush()\n\n...........................................................................\n/home/jeovach/.local/lib/python3.5/site-packages/ipykernel/ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code='from sklearn.grid_search import GridSearchCV\\nfro...     n_jobs=-1)\\ngs_lr_tfidf.fit(X_train, y_train)', silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    191 \n    192         self._forward_input(allow_stdin)\n    193 \n    194         reply_content = {}\n    195         try:\n--> 196             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = 'from sklearn.grid_search import GridSearchCV\\nfro...     n_jobs=-1)\\ngs_lr_tfidf.fit(X_train, y_train)'\n        store_history = True\n        silent = False\n    197         finally:\n    198             self._restore_input()\n    199 \n    200         if res.error_before_exec is not None:\n\n...........................................................................\n/home/jeovach/.local/lib/python3.5/site-packages/ipykernel/zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=('from sklearn.grid_search import GridSearchCV\\nfro...     n_jobs=-1)\\ngs_lr_tfidf.fit(X_train, y_train)',), **kwargs={'silent': False, 'store_history': True})\n    496             )\n    497         self.payload_manager.write_payload(payload)\n    498 \n    499     def run_cell(self, *args, **kwargs):\n    500         self._last_traceback = None\n--> 501         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = ('from sklearn.grid_search import GridSearchCV\\nfro...     n_jobs=-1)\\ngs_lr_tfidf.fit(X_train, y_train)',)\n        kwargs = {'silent': False, 'store_history': True}\n    502 \n    503     def _showtraceback(self, etype, evalue, stb):\n    504         # try to preserve ordering of tracebacks and print statements\n    505         sys.stdout.flush()\n\n...........................................................................\n/home/jeovach/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell='from sklearn.grid_search import GridSearchCV\\nfro...     n_jobs=-1)\\ngs_lr_tfidf.fit(X_train, y_train)', store_history=True, silent=False, shell_futures=True)\n   2712                 self.displayhook.exec_result = result\n   2713 \n   2714                 # Execute the user code\n   2715                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2716                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2717                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2718                 \n   2719                 self.last_execution_succeeded = not has_raised\n   2720 \n   2721                 # Reset this so later displayed values do not modify the\n\n...........................................................................\n/home/jeovach/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Expr object>], cell_name='<ipython-input-22-2f1bb946dc81>', interactivity='last', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 7fd2b050c6a0, executi..._before_exec=None error_in_exec=None result=None>)\n   2822                     return True\n   2823 \n   2824             for i, node in enumerate(to_run_interactive):\n   2825                 mod = ast.Interactive([node])\n   2826                 code = compiler(mod, cell_name, \"single\")\n-> 2827                 if self.run_code(code, result):\n        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x7fd2b0406a50, file \"<ipython-input-22-2f1bb946dc81>\", line 31>\n        result = <ExecutionResult object at 7fd2b050c6a0, executi..._before_exec=None error_in_exec=None result=None>\n   2828                     return True\n   2829 \n   2830             # Flush softspace\n   2831             if softspace(sys.stdout, 0):\n\n...........................................................................\n/home/jeovach/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x7fd2b0406a50, file \"<ipython-input-22-2f1bb946dc81>\", line 31>, result=<ExecutionResult object at 7fd2b050c6a0, executi..._before_exec=None error_in_exec=None result=None>)\n   2876         outflag = 1  # happens in more places, so it's easier as default\n   2877         try:\n   2878             try:\n   2879                 self.hooks.pre_run_code_hook()\n   2880                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2881                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x7fd2b0406a50, file \"<ipython-input-22-2f1bb946dc81>\", line 31>\n        self.user_global_ns = {'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'GridSearchCV': <class 'sklearn.grid_search.GridSearchCV'>, 'In': ['', 'import pandas as pd\\nimport pyprind\\nimport os\\npba... pbar.update()\\ndf.columns=[\"review\", \"sentiment\"]', 'import numpy as np\\nnp.random.seed(0)\\ndf = df.rei...lse)\\ndf = pd.read_csv(\"movie_data.csv\")\\ndf.head()', 'import pandas as pd\\ndf = pd.read_csv(\"movie_data.csv\")\\ndf.head()', 'import numpy as np\\nfrom sklearn.feature_extracti...cs)\\nprint(count.vocabulary_)\\nprint(bag.toarray())', 'from sklearn.feature_extraction.text import Tfid...t_transform(count.fit_transform(docs)).toarray())', 'import re\\ndef preprocessor(text):\\n    text = re....t\\ndf[\"review\"] = df[\"review\"].apply(preprocessor)', 'import numpy as np\\nnp.random.seed(0)\\ndf = df.rei...lse)\\ndf = pd.read_csv(\"movie_data.csv\")\\ndf.head()', 'import pandas as pd\\ndf = pd.read_csv(\"movie_data.csv\")\\ndf.head()', 'import numpy as np\\nfrom sklearn.feature_extracti...cs)\\nprint(count.vocabulary_)\\nprint(bag.toarray())', 'from sklearn.feature_extraction.text import Tfid...t_transform(count.fit_transform(docs)).toarray())', 'import re\\ndef preprocessor(text):\\n    text = re....t\\ndf[\"review\"] = df[\"review\"].apply(preprocessor)', 'import numpy as np\\nnp.random.seed(0)\\ndf = df.rei...sv\")\\ndf.columns=[\"review\", \"sentiment\"]\\ndf.head()', 'import pandas as pd\\ndf = pd.read_csv(\"movie_data.csv\")\\ndf.head()', 'import pandas as pd\\ndf = pd.read_csv(\"movie_data.csv\")\\ndf.columns=[\"review\", \"sentiment\"]\\ndf.head()', 'import numpy as np\\nfrom sklearn.feature_extracti...cs)\\nprint(count.vocabulary_)\\nprint(bag.toarray())', 'from sklearn.feature_extraction.text import Tfid...t_transform(count.fit_transform(docs)).toarray())', 'import re\\ndef preprocessor(text):\\n    text = re....t\\ndf[\"review\"] = df[\"review\"].apply(preprocessor)', 'def tokenizer(text):\\n    return text.split()', \"from nltk.stem.porter import PorterStemmer\\nporte..._porter('runners like running and thus they run')\", ...], 'LogisticRegression': <class 'sklearn.linear_model.logistic.LogisticRegression'>, 'Out': {2:                                                 ...occurred to me while watching \"Imaginary He...  1, 3:                                                 ...occurred to me while watching \"Imaginary He...  1, 7:                                                 .... could it be that ITV wouldn't want to rel...  1, 8:                                                 .... could it be that ITV wouldn't want to rel...  1, 12:                                               re...he movie was good, but I like to re...          1, 13:                                                 ...hought the movie was good, but I like to re...  1, 14:                                               re...he movie was good but i like to rea...          1, 19: ['runner', 'like', 'run', 'and', 'thu', 'they', 'run'], 20: ['runner', 'like', 'run', 'thu', 'run']}, 'Pipeline': <class 'sklearn.pipeline.Pipeline'>, 'PorterStemmer': <class 'nltk.stem.porter.PorterStemmer'>, 'TfidfTransformer': <class 'sklearn.feature_extraction.text.TfidfTransformer'>, 'TfidfVectorizer': <class 'sklearn.feature_extraction.text.TfidfVectorizer'>, 'X_test': array([], dtype=object), ...}\n        self.user_ns = {'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'GridSearchCV': <class 'sklearn.grid_search.GridSearchCV'>, 'In': ['', 'import pandas as pd\\nimport pyprind\\nimport os\\npba... pbar.update()\\ndf.columns=[\"review\", \"sentiment\"]', 'import numpy as np\\nnp.random.seed(0)\\ndf = df.rei...lse)\\ndf = pd.read_csv(\"movie_data.csv\")\\ndf.head()', 'import pandas as pd\\ndf = pd.read_csv(\"movie_data.csv\")\\ndf.head()', 'import numpy as np\\nfrom sklearn.feature_extracti...cs)\\nprint(count.vocabulary_)\\nprint(bag.toarray())', 'from sklearn.feature_extraction.text import Tfid...t_transform(count.fit_transform(docs)).toarray())', 'import re\\ndef preprocessor(text):\\n    text = re....t\\ndf[\"review\"] = df[\"review\"].apply(preprocessor)', 'import numpy as np\\nnp.random.seed(0)\\ndf = df.rei...lse)\\ndf = pd.read_csv(\"movie_data.csv\")\\ndf.head()', 'import pandas as pd\\ndf = pd.read_csv(\"movie_data.csv\")\\ndf.head()', 'import numpy as np\\nfrom sklearn.feature_extracti...cs)\\nprint(count.vocabulary_)\\nprint(bag.toarray())', 'from sklearn.feature_extraction.text import Tfid...t_transform(count.fit_transform(docs)).toarray())', 'import re\\ndef preprocessor(text):\\n    text = re....t\\ndf[\"review\"] = df[\"review\"].apply(preprocessor)', 'import numpy as np\\nnp.random.seed(0)\\ndf = df.rei...sv\")\\ndf.columns=[\"review\", \"sentiment\"]\\ndf.head()', 'import pandas as pd\\ndf = pd.read_csv(\"movie_data.csv\")\\ndf.head()', 'import pandas as pd\\ndf = pd.read_csv(\"movie_data.csv\")\\ndf.columns=[\"review\", \"sentiment\"]\\ndf.head()', 'import numpy as np\\nfrom sklearn.feature_extracti...cs)\\nprint(count.vocabulary_)\\nprint(bag.toarray())', 'from sklearn.feature_extraction.text import Tfid...t_transform(count.fit_transform(docs)).toarray())', 'import re\\ndef preprocessor(text):\\n    text = re....t\\ndf[\"review\"] = df[\"review\"].apply(preprocessor)', 'def tokenizer(text):\\n    return text.split()', \"from nltk.stem.porter import PorterStemmer\\nporte..._porter('runners like running and thus they run')\", ...], 'LogisticRegression': <class 'sklearn.linear_model.logistic.LogisticRegression'>, 'Out': {2:                                                 ...occurred to me while watching \"Imaginary He...  1, 3:                                                 ...occurred to me while watching \"Imaginary He...  1, 7:                                                 .... could it be that ITV wouldn't want to rel...  1, 8:                                                 .... could it be that ITV wouldn't want to rel...  1, 12:                                               re...he movie was good, but I like to re...          1, 13:                                                 ...hought the movie was good, but I like to re...  1, 14:                                               re...he movie was good but i like to rea...          1, 19: ['runner', 'like', 'run', 'and', 'thu', 'they', 'run'], 20: ['runner', 'like', 'run', 'thu', 'run']}, 'Pipeline': <class 'sklearn.pipeline.Pipeline'>, 'PorterStemmer': <class 'nltk.stem.porter.PorterStemmer'>, 'TfidfTransformer': <class 'sklearn.feature_extraction.text.TfidfTransformer'>, 'TfidfVectorizer': <class 'sklearn.feature_extraction.text.TfidfVectorizer'>, 'X_test': array([], dtype=object), ...}\n   2882             finally:\n   2883                 # Reset our crash handler in place\n   2884                 sys.excepthook = old_excepthook\n   2885         except SystemExit as e:\n\n...........................................................................\n/home/jeovach/python machine learning/<ipython-input-22-2f1bb946dc81> in <module>()\n     26                    )\n     27 gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n     28                           scoring=\"accuracy\",\n     29                           cv=5, verbose=1,\n     30                           n_jobs=-1)\n---> 31 gs_lr_tfidf.fit(X_train, y_train)\n     32 \n     33 \n     34 \n     35 \n\n...........................................................................\n/home/jeovach/anaconda3/lib/python3.5/site-packages/sklearn/grid_search.py in fit(self=GridSearchCV(cv=5, error_score='raise',\n       e...jobs', refit=True, scoring='accuracy', verbose=1), X=array([ 'i really love this movie i haven t play...to bust itself wide open D P D P'], dtype=object), y=array([1, 1, 1, ..., 1, 1, 1]))\n    824         y : array-like, shape = [n_samples] or [n_samples, n_output], optional\n    825             Target relative to X for classification or regression;\n    826             None for unsupervised learning.\n    827 \n    828         \"\"\"\n--> 829         return self._fit(X, y, ParameterGrid(self.param_grid))\n        self._fit = <bound method BaseSearchCV._fit of GridSearchCV(...obs', refit=True, scoring='accuracy', verbose=1)>\n        X = array([ 'i really love this movie i haven t play...to bust itself wide open D P D P'], dtype=object)\n        y = array([1, 1, 1, ..., 1, 1, 1])\n        self.param_grid = [{'clf__C': [1.0, 10.0, 100.0], 'clf__penalty': ['l1', 'l2'], 'vect__ngram_range': [(1, 1)], 'vect__stop_words': [['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], None], 'vect__tokenizer': [<function tokenizer>, <function tokenizer_porter>]}, {'clf__C': [1.0, 10.0, 100.0], 'clf__penalty': ['l1', 'l2'], 'vect__ngram_range': [(1, 1)], 'vect__norm': [None], 'vect__stop_words': [['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], None], 'vect__tokenizer': [<function tokenizer>, <function tokenizer_porter>], 'vect__use_idf': [False]}]\n    830 \n    831 \n    832 class RandomizedSearchCV(BaseSearchCV):\n    833     \"\"\"Randomized search on hyper parameters.\n\n...........................................................................\n/home/jeovach/anaconda3/lib/python3.5/site-packages/sklearn/grid_search.py in _fit(self=GridSearchCV(cv=5, error_score='raise',\n       e...jobs', refit=True, scoring='accuracy', verbose=1), X=array([ 'i really love this movie i haven t play...to bust itself wide open D P D P'], dtype=object), y=array([1, 1, 1, ..., 1, 1, 1]), parameter_iterable=<sklearn.grid_search.ParameterGrid object>)\n    568         )(\n    569             delayed(_fit_and_score)(clone(base_estimator), X, y, self.scorer_,\n    570                                     train, test, self.verbose, parameters,\n    571                                     self.fit_params, return_parameters=True,\n    572                                     error_score=self.error_score)\n--> 573                 for parameters in parameter_iterable\n        parameters = undefined\n        parameter_iterable = <sklearn.grid_search.ParameterGrid object>\n    574                 for train, test in cv)\n    575 \n    576         # Out is a list of triplet: score, estimator, n_test_samples\n    577         n_fits = len(out)\n\n...........................................................................\n/home/jeovach/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseSearchCV._fit.<locals>.<genexpr>>)\n    763             if pre_dispatch == \"all\" or n_jobs == 1:\n    764                 # The iterable was consumed all at once by the above for loop.\n    765                 # No need to wait for async callbacks to trigger to\n    766                 # consumption.\n    767                 self._iterating = False\n--> 768             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    769             # Make sure that we get a last message telling us we are done\n    770             elapsed_time = time.time() - self._start_time\n    771             self._print('Done %3i out of %3i | elapsed: %s finished',\n    772                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nValueError                                         Wed Mar  1 22:50:29 2017\nPID: 14881                 Python 3.5.2: /home/jeovach/anaconda3/bin/python\n...........................................................................\n/home/jeovach/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (Pipeline(steps=[('vect', TfidfVectorizer(analyze...0.0001,\n          verbose=0, warm_start=False))]), array([ 'i really love this movie i haven t play...to bust itself wide open D P D P'], dtype=object), array([1, 1, 1, ..., 1, 1, 1]), make_scorer(accuracy_score), array([ 650,  651,  652, ..., 3247, 3248, 3249]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ...40, 641, 642, 643, 644, 645, 646, 647, 648, 649]), 1, {'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer>}, {}), {'error_score': 'raise', 'return_parameters': True})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/home/jeovach/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (Pipeline(steps=[('vect', TfidfVectorizer(analyze...0.0001,\n          verbose=0, warm_start=False))]), array([ 'i really love this movie i haven t play...to bust itself wide open D P D P'], dtype=object), array([1, 1, 1, ..., 1, 1, 1]), make_scorer(accuracy_score), array([ 650,  651,  652, ..., 3247, 3248, 3249]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ...40, 641, 642, 643, 644, 645, 646, 647, 648, 649]), 1, {'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer>}, {})\n        kwargs = {'error_score': 'raise', 'return_parameters': True}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/home/jeovach/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py in _fit_and_score(estimator=Pipeline(steps=[('vect', TfidfVectorizer(analyze...0.0001,\n          verbose=0, warm_start=False))]), X=array([ 'i really love this movie i haven t play...to bust itself wide open D P D P'], dtype=object), y=array([1, 1, 1, ..., 1, 1, 1]), scorer=make_scorer(accuracy_score), train=array([ 650,  651,  652, ..., 3247, 3248, 3249]), test=array([  0,   1,   2,   3,   4,   5,   6,   7,  ...40, 641, 642, 643, 644, 645, 646, 647, 648, 649]), verbose=1, parameters={'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer>}, fit_params={}, return_train_score=False, return_parameters=True, error_score='raise')\n   1660 \n   1661     try:\n   1662         if y_train is None:\n   1663             estimator.fit(X_train, **fit_params)\n   1664         else:\n-> 1665             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method Pipeline.fit of Pipeline(steps=[('....0001,\n          verbose=0, warm_start=False))])>\n        X_train = array([ 'after a very disappointing part 3 i kin...to bust itself wide open D P D P'], dtype=object)\n        y_train = array([1, 1, 1, ..., 1, 1, 1])\n        fit_params = {}\n   1666 \n   1667     except Exception as e:\n   1668         if error_score == 'raise':\n   1669             raise\n\n...........................................................................\n/home/jeovach/anaconda3/lib/python3.5/site-packages/sklearn/pipeline.py in fit(self=Pipeline(steps=[('vect', TfidfVectorizer(analyze...0.0001,\n          verbose=0, warm_start=False))]), X=array([ 'after a very disappointing part 3 i kin...to bust itself wide open D P D P'], dtype=object), y=array([1, 1, 1, ..., 1, 1, 1]), **fit_params={})\n    265         self : Pipeline\n    266             This estimator\n    267         \"\"\"\n    268         Xt, fit_params = self._fit(X, y, **fit_params)\n    269         if self._final_estimator is not None:\n--> 270             self._final_estimator.fit(Xt, y, **fit_params)\n        self._final_estimator.fit = <bound method LogisticRegression.fit of Logistic...l=0.0001,\n          verbose=0, warm_start=False)>\n        Xt = <2600x28213 sparse matrix of type '<class 'numpy... stored elements in Compressed Sparse Row format>\n        y = array([1, 1, 1, ..., 1, 1, 1])\n        fit_params = {}\n    271         return self\n    272 \n    273     def fit_transform(self, X, y=None, **fit_params):\n    274         \"\"\"Fit the model and transform with the final estimator\n\n...........................................................................\n/home/jeovach/anaconda3/lib/python3.5/site-packages/sklearn/linear_model/logistic.py in fit(self=LogisticRegression(C=1.0, class_weight=None, dua...ol=0.0001,\n          verbose=0, warm_start=False), X=<2600x28213 sparse matrix of type '<class 'numpy... stored elements in Compressed Sparse Row format>, y=array([1, 1, 1, ..., 1, 1, 1]), sample_weight=None)\n   1181         if self.solver == 'liblinear':\n   1182             self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n   1183                 X, y, self.C, self.fit_intercept, self.intercept_scaling,\n   1184                 self.class_weight, self.penalty, self.dual, self.verbose,\n   1185                 self.max_iter, self.tol, self.random_state,\n-> 1186                 sample_weight=sample_weight)\n        sample_weight = None\n   1187             self.n_iter_ = np.array([n_iter_])\n   1188             return self\n   1189 \n   1190         if self.solver == 'sag':\n\n...........................................................................\n/home/jeovach/anaconda3/lib/python3.5/site-packages/sklearn/svm/base.py in _fit_liblinear(X=<2600x28213 sparse matrix of type '<class 'numpy... stored elements in Compressed Sparse Row format>, y=array([1, 1, 1, ..., 1, 1, 1]), C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, penalty='l1', dual=False, verbose=0, max_iter=100, tol=0.0001, random_state=0, multi_class='ovr', loss='logistic_regression', epsilon=0.1, sample_weight=None)\n    870         y_ind = enc.fit_transform(y)\n    871         classes_ = enc.classes_\n    872         if len(classes_) < 2:\n    873             raise ValueError(\"This solver needs samples of at least 2 classes\"\n    874                              \" in the data, but the data contains only one\"\n--> 875                              \" class: %r\" % classes_[0])\n        classes_ = array([1])\n    876 \n    877         class_weight_ = compute_class_weight(class_weight, classes_, y)\n    878     else:\n    879         class_weight_ = np.empty(0, dtype=np.float64)\n\nValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n___________________________________________________________________________",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/jeovach/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\", line 344, in __call__\n    return self.func(*args, **kwargs)\n  File \"/home/jeovach/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\", line 131, in __call__\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"/home/jeovach/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\", line 131, in <listcomp>\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"/home/jeovach/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py\", line 1665, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/home/jeovach/anaconda3/lib/python3.5/site-packages/sklearn/pipeline.py\", line 270, in fit\n    self._final_estimator.fit(Xt, y, **fit_params)\n  File \"/home/jeovach/anaconda3/lib/python3.5/site-packages/sklearn/linear_model/logistic.py\", line 1186, in fit\n    sample_weight=sample_weight)\n  File \"/home/jeovach/anaconda3/lib/python3.5/site-packages/sklearn/svm/base.py\", line 875, in _fit_liblinear\n    \" class: %r\" % classes_[0])\nValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/jeovach/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/home/jeovach/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\", line 353, in __call__\n    raise TransportableException(text, e_type)\nsklearn.externals.joblib.my_exceptions.TransportableException: TransportableException\n___________________________________________________________________________\nValueError                                         Wed Mar  1 22:50:29 2017\nPID: 14881                 Python 3.5.2: /home/jeovach/anaconda3/bin/python\n...........................................................................\n/home/jeovach/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (Pipeline(steps=[('vect', TfidfVectorizer(analyze...0.0001,\n          verbose=0, warm_start=False))]), array([ 'i really love this movie i haven t play...to bust itself wide open D P D P'], dtype=object), array([1, 1, 1, ..., 1, 1, 1]), make_scorer(accuracy_score), array([ 650,  651,  652, ..., 3247, 3248, 3249]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ...40, 641, 642, 643, 644, 645, 646, 647, 648, 649]), 1, {'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer>}, {}), {'error_score': 'raise', 'return_parameters': True})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/home/jeovach/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (Pipeline(steps=[('vect', TfidfVectorizer(analyze...0.0001,\n          verbose=0, warm_start=False))]), array([ 'i really love this movie i haven t play...to bust itself wide open D P D P'], dtype=object), array([1, 1, 1, ..., 1, 1, 1]), make_scorer(accuracy_score), array([ 650,  651,  652, ..., 3247, 3248, 3249]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ...40, 641, 642, 643, 644, 645, 646, 647, 648, 649]), 1, {'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer>}, {})\n        kwargs = {'error_score': 'raise', 'return_parameters': True}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/home/jeovach/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py in _fit_and_score(estimator=Pipeline(steps=[('vect', TfidfVectorizer(analyze...0.0001,\n          verbose=0, warm_start=False))]), X=array([ 'i really love this movie i haven t play...to bust itself wide open D P D P'], dtype=object), y=array([1, 1, 1, ..., 1, 1, 1]), scorer=make_scorer(accuracy_score), train=array([ 650,  651,  652, ..., 3247, 3248, 3249]), test=array([  0,   1,   2,   3,   4,   5,   6,   7,  ...40, 641, 642, 643, 644, 645, 646, 647, 648, 649]), verbose=1, parameters={'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer>}, fit_params={}, return_train_score=False, return_parameters=True, error_score='raise')\n   1660 \n   1661     try:\n   1662         if y_train is None:\n   1663             estimator.fit(X_train, **fit_params)\n   1664         else:\n-> 1665             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method Pipeline.fit of Pipeline(steps=[('....0001,\n          verbose=0, warm_start=False))])>\n        X_train = array([ 'after a very disappointing part 3 i kin...to bust itself wide open D P D P'], dtype=object)\n        y_train = array([1, 1, 1, ..., 1, 1, 1])\n        fit_params = {}\n   1666 \n   1667     except Exception as e:\n   1668         if error_score == 'raise':\n   1669             raise\n\n...........................................................................\n/home/jeovach/anaconda3/lib/python3.5/site-packages/sklearn/pipeline.py in fit(self=Pipeline(steps=[('vect', TfidfVectorizer(analyze...0.0001,\n          verbose=0, warm_start=False))]), X=array([ 'after a very disappointing part 3 i kin...to bust itself wide open D P D P'], dtype=object), y=array([1, 1, 1, ..., 1, 1, 1]), **fit_params={})\n    265         self : Pipeline\n    266             This estimator\n    267         \"\"\"\n    268         Xt, fit_params = self._fit(X, y, **fit_params)\n    269         if self._final_estimator is not None:\n--> 270             self._final_estimator.fit(Xt, y, **fit_params)\n        self._final_estimator.fit = <bound method LogisticRegression.fit of Logistic...l=0.0001,\n          verbose=0, warm_start=False)>\n        Xt = <2600x28213 sparse matrix of type '<class 'numpy... stored elements in Compressed Sparse Row format>\n        y = array([1, 1, 1, ..., 1, 1, 1])\n        fit_params = {}\n    271         return self\n    272 \n    273     def fit_transform(self, X, y=None, **fit_params):\n    274         \"\"\"Fit the model and transform with the final estimator\n\n...........................................................................\n/home/jeovach/anaconda3/lib/python3.5/site-packages/sklearn/linear_model/logistic.py in fit(self=LogisticRegression(C=1.0, class_weight=None, dua...ol=0.0001,\n          verbose=0, warm_start=False), X=<2600x28213 sparse matrix of type '<class 'numpy... stored elements in Compressed Sparse Row format>, y=array([1, 1, 1, ..., 1, 1, 1]), sample_weight=None)\n   1181         if self.solver == 'liblinear':\n   1182             self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n   1183                 X, y, self.C, self.fit_intercept, self.intercept_scaling,\n   1184                 self.class_weight, self.penalty, self.dual, self.verbose,\n   1185                 self.max_iter, self.tol, self.random_state,\n-> 1186                 sample_weight=sample_weight)\n        sample_weight = None\n   1187             self.n_iter_ = np.array([n_iter_])\n   1188             return self\n   1189 \n   1190         if self.solver == 'sag':\n\n...........................................................................\n/home/jeovach/anaconda3/lib/python3.5/site-packages/sklearn/svm/base.py in _fit_liblinear(X=<2600x28213 sparse matrix of type '<class 'numpy... stored elements in Compressed Sparse Row format>, y=array([1, 1, 1, ..., 1, 1, 1]), C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, penalty='l1', dual=False, verbose=0, max_iter=100, tol=0.0001, random_state=0, multi_class='ovr', loss='logistic_regression', epsilon=0.1, sample_weight=None)\n    870         y_ind = enc.fit_transform(y)\n    871         classes_ = enc.classes_\n    872         if len(classes_) < 2:\n    873             raise ValueError(\"This solver needs samples of at least 2 classes\"\n    874                              \" in the data, but the data contains only one\"\n--> 875                              \" class: %r\" % classes_[0])\n        classes_ = array([1])\n    876 \n    877         class_weight_ = compute_class_weight(class_weight, classes_, y)\n    878     else:\n    879         class_weight_ = np.empty(0, dtype=np.float64)\n\nValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n___________________________________________________________________________\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTransportableException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/home/jeovach/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    681\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m'timeout'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetfullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jeovach/anaconda3/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTransportableException\u001b[0m: TransportableException\n___________________________________________________________________________\nValueError                                         Wed Mar  1 22:50:29 2017\nPID: 14881                 Python 3.5.2: /home/jeovach/anaconda3/bin/python\n...........................................................................\n/home/jeovach/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (Pipeline(steps=[('vect', TfidfVectorizer(analyze...0.0001,\n          verbose=0, warm_start=False))]), array([ 'i really love this movie i haven t play...to bust itself wide open D P D P'], dtype=object), array([1, 1, 1, ..., 1, 1, 1]), make_scorer(accuracy_score), array([ 650,  651,  652, ..., 3247, 3248, 3249]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ...40, 641, 642, 643, 644, 645, 646, 647, 648, 649]), 1, {'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer>}, {}), {'error_score': 'raise', 'return_parameters': True})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/home/jeovach/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (Pipeline(steps=[('vect', TfidfVectorizer(analyze...0.0001,\n          verbose=0, warm_start=False))]), array([ 'i really love this movie i haven t play...to bust itself wide open D P D P'], dtype=object), array([1, 1, 1, ..., 1, 1, 1]), make_scorer(accuracy_score), array([ 650,  651,  652, ..., 3247, 3248, 3249]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ...40, 641, 642, 643, 644, 645, 646, 647, 648, 649]), 1, {'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer>}, {})\n        kwargs = {'error_score': 'raise', 'return_parameters': True}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/home/jeovach/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py in _fit_and_score(estimator=Pipeline(steps=[('vect', TfidfVectorizer(analyze...0.0001,\n          verbose=0, warm_start=False))]), X=array([ 'i really love this movie i haven t play...to bust itself wide open D P D P'], dtype=object), y=array([1, 1, 1, ..., 1, 1, 1]), scorer=make_scorer(accuracy_score), train=array([ 650,  651,  652, ..., 3247, 3248, 3249]), test=array([  0,   1,   2,   3,   4,   5,   6,   7,  ...40, 641, 642, 643, 644, 645, 646, 647, 648, 649]), verbose=1, parameters={'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer>}, fit_params={}, return_train_score=False, return_parameters=True, error_score='raise')\n   1660 \n   1661     try:\n   1662         if y_train is None:\n   1663             estimator.fit(X_train, **fit_params)\n   1664         else:\n-> 1665             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method Pipeline.fit of Pipeline(steps=[('....0001,\n          verbose=0, warm_start=False))])>\n        X_train = array([ 'after a very disappointing part 3 i kin...to bust itself wide open D P D P'], dtype=object)\n        y_train = array([1, 1, 1, ..., 1, 1, 1])\n        fit_params = {}\n   1666 \n   1667     except Exception as e:\n   1668         if error_score == 'raise':\n   1669             raise\n\n...........................................................................\n/home/jeovach/anaconda3/lib/python3.5/site-packages/sklearn/pipeline.py in fit(self=Pipeline(steps=[('vect', TfidfVectorizer(analyze...0.0001,\n          verbose=0, warm_start=False))]), X=array([ 'after a very disappointing part 3 i kin...to bust itself wide open D P D P'], dtype=object), y=array([1, 1, 1, ..., 1, 1, 1]), **fit_params={})\n    265         self : Pipeline\n    266             This estimator\n    267         \"\"\"\n    268         Xt, fit_params = self._fit(X, y, **fit_params)\n    269         if self._final_estimator is not None:\n--> 270             self._final_estimator.fit(Xt, y, **fit_params)\n        self._final_estimator.fit = <bound method LogisticRegression.fit of Logistic...l=0.0001,\n          verbose=0, warm_start=False)>\n        Xt = <2600x28213 sparse matrix of type '<class 'numpy... stored elements in Compressed Sparse Row format>\n        y = array([1, 1, 1, ..., 1, 1, 1])\n        fit_params = {}\n    271         return self\n    272 \n    273     def fit_transform(self, X, y=None, **fit_params):\n    274         \"\"\"Fit the model and transform with the final estimator\n\n...........................................................................\n/home/jeovach/anaconda3/lib/python3.5/site-packages/sklearn/linear_model/logistic.py in fit(self=LogisticRegression(C=1.0, class_weight=None, dua...ol=0.0001,\n          verbose=0, warm_start=False), X=<2600x28213 sparse matrix of type '<class 'numpy... stored elements in Compressed Sparse Row format>, y=array([1, 1, 1, ..., 1, 1, 1]), sample_weight=None)\n   1181         if self.solver == 'liblinear':\n   1182             self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n   1183                 X, y, self.C, self.fit_intercept, self.intercept_scaling,\n   1184                 self.class_weight, self.penalty, self.dual, self.verbose,\n   1185                 self.max_iter, self.tol, self.random_state,\n-> 1186                 sample_weight=sample_weight)\n        sample_weight = None\n   1187             self.n_iter_ = np.array([n_iter_])\n   1188             return self\n   1189 \n   1190         if self.solver == 'sag':\n\n...........................................................................\n/home/jeovach/anaconda3/lib/python3.5/site-packages/sklearn/svm/base.py in _fit_liblinear(X=<2600x28213 sparse matrix of type '<class 'numpy... stored elements in Compressed Sparse Row format>, y=array([1, 1, 1, ..., 1, 1, 1]), C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, penalty='l1', dual=False, verbose=0, max_iter=100, tol=0.0001, random_state=0, multi_class='ovr', loss='logistic_regression', epsilon=0.1, sample_weight=None)\n    870         y_ind = enc.fit_transform(y)\n    871         classes_ = enc.classes_\n    872         if len(classes_) < 2:\n    873             raise ValueError(\"This solver needs samples of at least 2 classes\"\n    874                              \" in the data, but the data contains only one\"\n--> 875                              \" class: %r\" % classes_[0])\n        classes_ = array([1])\n    876 \n    877         class_weight_ = compute_class_weight(class_weight, classes_, y)\n    878     else:\n    879         class_weight_ = np.empty(0, dtype=np.float64)\n\nValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n___________________________________________________________________________",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mJoblibValueError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-2f1bb946dc81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m                           \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                           n_jobs=-1)\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mgs_lr_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/jeovach/anaconda3/lib/python3.5/site-packages/sklearn/grid_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m         \"\"\"\n\u001b[0;32m--> 829\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jeovach/anaconda3/lib/python3.5/site-packages/sklearn/grid_search.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[1;32m    571\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m                                     error_score=self.error_score)\n\u001b[0;32m--> 573\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m                 for train, test in cv)\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jeovach/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    766\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jeovach/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJoblibValueError\u001b[0m: JoblibValueError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\n/home/jeovach/anaconda3/lib/python3.5/runpy.py in _run_module_as_main(mod_name='ipykernel.__main__', alter_argv=1)\n    179         sys.exit(msg)\n    180     main_globals = sys.modules[\"__main__\"].__dict__\n    181     if alter_argv:\n    182         sys.argv[0] = mod_spec.origin\n    183     return _run_code(code, main_globals, None,\n--> 184                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel.__main__', loader=<_f...b/python3.5/site-packages/ipykernel/__main__.py')\n    185 \n    186 def run_module(mod_name, init_globals=None,\n    187                run_name=None, alter_sys=False):\n    188     \"\"\"Execute a module's code without importing it\n\n...........................................................................\n/home/jeovach/anaconda3/lib/python3.5/runpy.py in _run_code(code=<code object <module> at 0x7fd2f2be24b0, file \"/...3.5/site-packages/ipykernel/__main__.py\", line 1>, run_globals={'__builtins__': <module 'builtins' (built-in)>, '__cached__': '/home/jeovach/.local/lib/python3.5/site-packages/ipykernel/__pycache__/__main__.cpython-35.pyc', '__doc__': None, '__file__': '/home/jeovach/.local/lib/python3.5/site-packages/ipykernel/__main__.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': 'ipykernel', '__spec__': ModuleSpec(name='ipykernel.__main__', loader=<_f...b/python3.5/site-packages/ipykernel/__main__.py'), 'app': <module 'ipykernel.kernelapp' from '/home/jeovac.../python3.5/site-packages/ipykernel/kernelapp.py'>}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel.__main__', loader=<_f...b/python3.5/site-packages/ipykernel/__main__.py'), pkg_name='ipykernel', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x7fd2f2be24b0, file \"/...3.5/site-packages/ipykernel/__main__.py\", line 1>\n        run_globals = {'__builtins__': <module 'builtins' (built-in)>, '__cached__': '/home/jeovach/.local/lib/python3.5/site-packages/ipykernel/__pycache__/__main__.cpython-35.pyc', '__doc__': None, '__file__': '/home/jeovach/.local/lib/python3.5/site-packages/ipykernel/__main__.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': 'ipykernel', '__spec__': ModuleSpec(name='ipykernel.__main__', loader=<_f...b/python3.5/site-packages/ipykernel/__main__.py'), 'app': <module 'ipykernel.kernelapp' from '/home/jeovac.../python3.5/site-packages/ipykernel/kernelapp.py'>}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\n/home/jeovach/.local/lib/python3.5/site-packages/ipykernel/__main__.py in <module>()\n      1 \n      2 \n----> 3 \n      4 if __name__ == '__main__':\n      5     from ipykernel import kernelapp as app\n      6     app.launch_new_instance()\n      7 \n      8 \n      9 \n     10 \n\n...........................................................................\n/home/jeovach/.local/lib/python3.5/site-packages/traitlets/config/application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\n/home/jeovach/.local/lib/python3.5/site-packages/ipykernel/kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    469             return self.subapp.start()\n    470         if self.poller is not None:\n    471             self.poller.start()\n    472         self.kernel.start()\n    473         try:\n--> 474             ioloop.IOLoop.instance().start()\n    475         except KeyboardInterrupt:\n    476             pass\n    477 \n    478 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\n/home/jeovach/.local/lib/python3.5/site-packages/zmq/eventloop/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    172             )\n    173         return loop\n    174     \n    175     def start(self):\n    176         try:\n--> 177             super(ZMQIOLoop, self).start()\n        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    178         except ZMQError as e:\n    179             if e.errno == ETERM:\n    180                 # quietly return on ETERM\n    181                 pass\n\n...........................................................................\n/home/jeovach/.local/lib/python3.5/site-packages/tornado/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    882                 self._events.update(event_pairs)\n    883                 while self._events:\n    884                     fd, events = self._events.popitem()\n    885                     try:\n    886                         fd_obj, handler_func = self._handlers[fd]\n--> 887                         handler_func(fd_obj, events)\n        handler_func = <function wrap.<locals>.null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 1\n    888                     except (OSError, IOError) as e:\n    889                         if errno_from_exception(e) == errno.EPIPE:\n    890                             # Happens when the client closes the connection\n    891                             pass\n\n...........................................................................\n/home/jeovach/.local/lib/python3.5/site-packages/tornado/stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    270         # Fast path when there are no active contexts.\n    271         def null_wrapper(*args, **kwargs):\n    272             try:\n    273                 current_state = _state.contexts\n    274                 _state.contexts = cap_contexts[0]\n--> 275                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    276             finally:\n    277                 _state.contexts = current_state\n    278         null_wrapper._wrapped = True\n    279         return null_wrapper\n\n...........................................................................\n/home/jeovach/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    435             # dispatch events:\n    436             if events & IOLoop.ERROR:\n    437                 gen_log.error(\"got POLLERR event on ZMQStream, which doesn't make sense\")\n    438                 return\n    439             if events & IOLoop.READ:\n--> 440                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    441                 if not self.socket:\n    442                     return\n    443             if events & IOLoop.WRITE:\n    444                 self._handle_send()\n\n...........................................................................\n/home/jeovach/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    467                 gen_log.error(\"RECV Error: %s\"%zmq.strerror(e.errno))\n    468         else:\n    469             if self._recv_callback:\n    470                 callback = self._recv_callback\n    471                 # self._recv_callback = None\n--> 472                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    473                 \n    474         # self.update_state()\n    475         \n    476 \n\n...........................................................................\n/home/jeovach/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    409         close our socket.\"\"\"\n    410         try:\n    411             # Use a NullContext to ensure that all StackContexts are run\n    412             # inside our blanket exception handler rather than outside.\n    413             with stack_context.NullContext():\n--> 414                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    415         except:\n    416             gen_log.error(\"Uncaught exception, closing connection.\",\n    417                           exc_info=True)\n    418             # Close the socket on an uncaught exception from a user callback\n\n...........................................................................\n/home/jeovach/.local/lib/python3.5/site-packages/tornado/stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    270         # Fast path when there are no active contexts.\n    271         def null_wrapper(*args, **kwargs):\n    272             try:\n    273                 current_state = _state.contexts\n    274                 _state.contexts = cap_contexts[0]\n--> 275                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    276             finally:\n    277                 _state.contexts = current_state\n    278         null_wrapper._wrapped = True\n    279         return null_wrapper\n\n...........................................................................\n/home/jeovach/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    271         if self.control_stream:\n    272             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    273 \n    274         def make_dispatcher(stream):\n    275             def dispatcher(msg):\n--> 276                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    277             return dispatcher\n    278 \n    279         for s in self.shell_streams:\n    280             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\n/home/jeovach/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': 'from sklearn.grid_search import GridSearchCV\\nfro...     n_jobs=-1)\\ngs_lr_tfidf.fit(X_train, y_train)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': '2017-03-01T22:50:28.287994', 'msg_id': '4EE484EE23394C359C3E459091E4216B', 'msg_type': 'execute_request', 'session': '9C093F067364436688AACCCE8EB0BEA3', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '4EE484EE23394C359C3E459091E4216B', 'msg_type': 'execute_request', 'parent_header': {}})\n    223             self.log.error(\"UNKNOWN MESSAGE TYPE: %r\", msg_type)\n    224         else:\n    225             self.log.debug(\"%s: %s\", msg_type, msg)\n    226             self.pre_handler_hook()\n    227             try:\n--> 228                 handler(stream, idents, msg)\n        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'9C093F067364436688AACCCE8EB0BEA3']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': 'from sklearn.grid_search import GridSearchCV\\nfro...     n_jobs=-1)\\ngs_lr_tfidf.fit(X_train, y_train)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': '2017-03-01T22:50:28.287994', 'msg_id': '4EE484EE23394C359C3E459091E4216B', 'msg_type': 'execute_request', 'session': '9C093F067364436688AACCCE8EB0BEA3', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '4EE484EE23394C359C3E459091E4216B', 'msg_type': 'execute_request', 'parent_header': {}}\n    229             except Exception:\n    230                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    231             finally:\n    232                 self.post_handler_hook()\n\n...........................................................................\n/home/jeovach/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'9C093F067364436688AACCCE8EB0BEA3'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': 'from sklearn.grid_search import GridSearchCV\\nfro...     n_jobs=-1)\\ngs_lr_tfidf.fit(X_train, y_train)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': '2017-03-01T22:50:28.287994', 'msg_id': '4EE484EE23394C359C3E459091E4216B', 'msg_type': 'execute_request', 'session': '9C093F067364436688AACCCE8EB0BEA3', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '4EE484EE23394C359C3E459091E4216B', 'msg_type': 'execute_request', 'parent_header': {}})\n    385         if not silent:\n    386             self.execution_count += 1\n    387             self._publish_execute_input(code, parent, self.execution_count)\n    388 \n    389         reply_content = self.do_execute(code, silent, store_history,\n--> 390                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    391 \n    392         # Flush output before sending the reply.\n    393         sys.stdout.flush()\n    394         sys.stderr.flush()\n\n...........................................................................\n/home/jeovach/.local/lib/python3.5/site-packages/ipykernel/ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code='from sklearn.grid_search import GridSearchCV\\nfro...     n_jobs=-1)\\ngs_lr_tfidf.fit(X_train, y_train)', silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    191 \n    192         self._forward_input(allow_stdin)\n    193 \n    194         reply_content = {}\n    195         try:\n--> 196             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = 'from sklearn.grid_search import GridSearchCV\\nfro...     n_jobs=-1)\\ngs_lr_tfidf.fit(X_train, y_train)'\n        store_history = True\n        silent = False\n    197         finally:\n    198             self._restore_input()\n    199 \n    200         if res.error_before_exec is not None:\n\n...........................................................................\n/home/jeovach/.local/lib/python3.5/site-packages/ipykernel/zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=('from sklearn.grid_search import GridSearchCV\\nfro...     n_jobs=-1)\\ngs_lr_tfidf.fit(X_train, y_train)',), **kwargs={'silent': False, 'store_history': True})\n    496             )\n    497         self.payload_manager.write_payload(payload)\n    498 \n    499     def run_cell(self, *args, **kwargs):\n    500         self._last_traceback = None\n--> 501         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = ('from sklearn.grid_search import GridSearchCV\\nfro...     n_jobs=-1)\\ngs_lr_tfidf.fit(X_train, y_train)',)\n        kwargs = {'silent': False, 'store_history': True}\n    502 \n    503     def _showtraceback(self, etype, evalue, stb):\n    504         # try to preserve ordering of tracebacks and print statements\n    505         sys.stdout.flush()\n\n...........................................................................\n/home/jeovach/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell='from sklearn.grid_search import GridSearchCV\\nfro...     n_jobs=-1)\\ngs_lr_tfidf.fit(X_train, y_train)', store_history=True, silent=False, shell_futures=True)\n   2712                 self.displayhook.exec_result = result\n   2713 \n   2714                 # Execute the user code\n   2715                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2716                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2717                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2718                 \n   2719                 self.last_execution_succeeded = not has_raised\n   2720 \n   2721                 # Reset this so later displayed values do not modify the\n\n...........................................................................\n/home/jeovach/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Expr object>], cell_name='<ipython-input-22-2f1bb946dc81>', interactivity='last', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 7fd2b050c6a0, executi..._before_exec=None error_in_exec=None result=None>)\n   2822                     return True\n   2823 \n   2824             for i, node in enumerate(to_run_interactive):\n   2825                 mod = ast.Interactive([node])\n   2826                 code = compiler(mod, cell_name, \"single\")\n-> 2827                 if self.run_code(code, result):\n        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x7fd2b0406a50, file \"<ipython-input-22-2f1bb946dc81>\", line 31>\n        result = <ExecutionResult object at 7fd2b050c6a0, executi..._before_exec=None error_in_exec=None result=None>\n   2828                     return True\n   2829 \n   2830             # Flush softspace\n   2831             if softspace(sys.stdout, 0):\n\n...........................................................................\n/home/jeovach/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x7fd2b0406a50, file \"<ipython-input-22-2f1bb946dc81>\", line 31>, result=<ExecutionResult object at 7fd2b050c6a0, executi..._before_exec=None error_in_exec=None result=None>)\n   2876         outflag = 1  # happens in more places, so it's easier as default\n   2877         try:\n   2878             try:\n   2879                 self.hooks.pre_run_code_hook()\n   2880                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2881                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x7fd2b0406a50, file \"<ipython-input-22-2f1bb946dc81>\", line 31>\n        self.user_global_ns = {'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'GridSearchCV': <class 'sklearn.grid_search.GridSearchCV'>, 'In': ['', 'import pandas as pd\\nimport pyprind\\nimport os\\npba... pbar.update()\\ndf.columns=[\"review\", \"sentiment\"]', 'import numpy as np\\nnp.random.seed(0)\\ndf = df.rei...lse)\\ndf = pd.read_csv(\"movie_data.csv\")\\ndf.head()', 'import pandas as pd\\ndf = pd.read_csv(\"movie_data.csv\")\\ndf.head()', 'import numpy as np\\nfrom sklearn.feature_extracti...cs)\\nprint(count.vocabulary_)\\nprint(bag.toarray())', 'from sklearn.feature_extraction.text import Tfid...t_transform(count.fit_transform(docs)).toarray())', 'import re\\ndef preprocessor(text):\\n    text = re....t\\ndf[\"review\"] = df[\"review\"].apply(preprocessor)', 'import numpy as np\\nnp.random.seed(0)\\ndf = df.rei...lse)\\ndf = pd.read_csv(\"movie_data.csv\")\\ndf.head()', 'import pandas as pd\\ndf = pd.read_csv(\"movie_data.csv\")\\ndf.head()', 'import numpy as np\\nfrom sklearn.feature_extracti...cs)\\nprint(count.vocabulary_)\\nprint(bag.toarray())', 'from sklearn.feature_extraction.text import Tfid...t_transform(count.fit_transform(docs)).toarray())', 'import re\\ndef preprocessor(text):\\n    text = re....t\\ndf[\"review\"] = df[\"review\"].apply(preprocessor)', 'import numpy as np\\nnp.random.seed(0)\\ndf = df.rei...sv\")\\ndf.columns=[\"review\", \"sentiment\"]\\ndf.head()', 'import pandas as pd\\ndf = pd.read_csv(\"movie_data.csv\")\\ndf.head()', 'import pandas as pd\\ndf = pd.read_csv(\"movie_data.csv\")\\ndf.columns=[\"review\", \"sentiment\"]\\ndf.head()', 'import numpy as np\\nfrom sklearn.feature_extracti...cs)\\nprint(count.vocabulary_)\\nprint(bag.toarray())', 'from sklearn.feature_extraction.text import Tfid...t_transform(count.fit_transform(docs)).toarray())', 'import re\\ndef preprocessor(text):\\n    text = re....t\\ndf[\"review\"] = df[\"review\"].apply(preprocessor)', 'def tokenizer(text):\\n    return text.split()', \"from nltk.stem.porter import PorterStemmer\\nporte..._porter('runners like running and thus they run')\", ...], 'LogisticRegression': <class 'sklearn.linear_model.logistic.LogisticRegression'>, 'Out': {2:                                                 ...occurred to me while watching \"Imaginary He...  1, 3:                                                 ...occurred to me while watching \"Imaginary He...  1, 7:                                                 .... could it be that ITV wouldn't want to rel...  1, 8:                                                 .... could it be that ITV wouldn't want to rel...  1, 12:                                               re...he movie was good, but I like to re...          1, 13:                                                 ...hought the movie was good, but I like to re...  1, 14:                                               re...he movie was good but i like to rea...          1, 19: ['runner', 'like', 'run', 'and', 'thu', 'they', 'run'], 20: ['runner', 'like', 'run', 'thu', 'run']}, 'Pipeline': <class 'sklearn.pipeline.Pipeline'>, 'PorterStemmer': <class 'nltk.stem.porter.PorterStemmer'>, 'TfidfTransformer': <class 'sklearn.feature_extraction.text.TfidfTransformer'>, 'TfidfVectorizer': <class 'sklearn.feature_extraction.text.TfidfVectorizer'>, 'X_test': array([], dtype=object), ...}\n        self.user_ns = {'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'GridSearchCV': <class 'sklearn.grid_search.GridSearchCV'>, 'In': ['', 'import pandas as pd\\nimport pyprind\\nimport os\\npba... pbar.update()\\ndf.columns=[\"review\", \"sentiment\"]', 'import numpy as np\\nnp.random.seed(0)\\ndf = df.rei...lse)\\ndf = pd.read_csv(\"movie_data.csv\")\\ndf.head()', 'import pandas as pd\\ndf = pd.read_csv(\"movie_data.csv\")\\ndf.head()', 'import numpy as np\\nfrom sklearn.feature_extracti...cs)\\nprint(count.vocabulary_)\\nprint(bag.toarray())', 'from sklearn.feature_extraction.text import Tfid...t_transform(count.fit_transform(docs)).toarray())', 'import re\\ndef preprocessor(text):\\n    text = re....t\\ndf[\"review\"] = df[\"review\"].apply(preprocessor)', 'import numpy as np\\nnp.random.seed(0)\\ndf = df.rei...lse)\\ndf = pd.read_csv(\"movie_data.csv\")\\ndf.head()', 'import pandas as pd\\ndf = pd.read_csv(\"movie_data.csv\")\\ndf.head()', 'import numpy as np\\nfrom sklearn.feature_extracti...cs)\\nprint(count.vocabulary_)\\nprint(bag.toarray())', 'from sklearn.feature_extraction.text import Tfid...t_transform(count.fit_transform(docs)).toarray())', 'import re\\ndef preprocessor(text):\\n    text = re....t\\ndf[\"review\"] = df[\"review\"].apply(preprocessor)', 'import numpy as np\\nnp.random.seed(0)\\ndf = df.rei...sv\")\\ndf.columns=[\"review\", \"sentiment\"]\\ndf.head()', 'import pandas as pd\\ndf = pd.read_csv(\"movie_data.csv\")\\ndf.head()', 'import pandas as pd\\ndf = pd.read_csv(\"movie_data.csv\")\\ndf.columns=[\"review\", \"sentiment\"]\\ndf.head()', 'import numpy as np\\nfrom sklearn.feature_extracti...cs)\\nprint(count.vocabulary_)\\nprint(bag.toarray())', 'from sklearn.feature_extraction.text import Tfid...t_transform(count.fit_transform(docs)).toarray())', 'import re\\ndef preprocessor(text):\\n    text = re....t\\ndf[\"review\"] = df[\"review\"].apply(preprocessor)', 'def tokenizer(text):\\n    return text.split()', \"from nltk.stem.porter import PorterStemmer\\nporte..._porter('runners like running and thus they run')\", ...], 'LogisticRegression': <class 'sklearn.linear_model.logistic.LogisticRegression'>, 'Out': {2:                                                 ...occurred to me while watching \"Imaginary He...  1, 3:                                                 ...occurred to me while watching \"Imaginary He...  1, 7:                                                 .... could it be that ITV wouldn't want to rel...  1, 8:                                                 .... could it be that ITV wouldn't want to rel...  1, 12:                                               re...he movie was good, but I like to re...          1, 13:                                                 ...hought the movie was good, but I like to re...  1, 14:                                               re...he movie was good but i like to rea...          1, 19: ['runner', 'like', 'run', 'and', 'thu', 'they', 'run'], 20: ['runner', 'like', 'run', 'thu', 'run']}, 'Pipeline': <class 'sklearn.pipeline.Pipeline'>, 'PorterStemmer': <class 'nltk.stem.porter.PorterStemmer'>, 'TfidfTransformer': <class 'sklearn.feature_extraction.text.TfidfTransformer'>, 'TfidfVectorizer': <class 'sklearn.feature_extraction.text.TfidfVectorizer'>, 'X_test': array([], dtype=object), ...}\n   2882             finally:\n   2883                 # Reset our crash handler in place\n   2884                 sys.excepthook = old_excepthook\n   2885         except SystemExit as e:\n\n...........................................................................\n/home/jeovach/python machine learning/<ipython-input-22-2f1bb946dc81> in <module>()\n     26                    )\n     27 gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n     28                           scoring=\"accuracy\",\n     29                           cv=5, verbose=1,\n     30                           n_jobs=-1)\n---> 31 gs_lr_tfidf.fit(X_train, y_train)\n     32 \n     33 \n     34 \n     35 \n\n...........................................................................\n/home/jeovach/anaconda3/lib/python3.5/site-packages/sklearn/grid_search.py in fit(self=GridSearchCV(cv=5, error_score='raise',\n       e...jobs', refit=True, scoring='accuracy', verbose=1), X=array([ 'i really love this movie i haven t play...to bust itself wide open D P D P'], dtype=object), y=array([1, 1, 1, ..., 1, 1, 1]))\n    824         y : array-like, shape = [n_samples] or [n_samples, n_output], optional\n    825             Target relative to X for classification or regression;\n    826             None for unsupervised learning.\n    827 \n    828         \"\"\"\n--> 829         return self._fit(X, y, ParameterGrid(self.param_grid))\n        self._fit = <bound method BaseSearchCV._fit of GridSearchCV(...obs', refit=True, scoring='accuracy', verbose=1)>\n        X = array([ 'i really love this movie i haven t play...to bust itself wide open D P D P'], dtype=object)\n        y = array([1, 1, 1, ..., 1, 1, 1])\n        self.param_grid = [{'clf__C': [1.0, 10.0, 100.0], 'clf__penalty': ['l1', 'l2'], 'vect__ngram_range': [(1, 1)], 'vect__stop_words': [['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], None], 'vect__tokenizer': [<function tokenizer>, <function tokenizer_porter>]}, {'clf__C': [1.0, 10.0, 100.0], 'clf__penalty': ['l1', 'l2'], 'vect__ngram_range': [(1, 1)], 'vect__norm': [None], 'vect__stop_words': [['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], None], 'vect__tokenizer': [<function tokenizer>, <function tokenizer_porter>], 'vect__use_idf': [False]}]\n    830 \n    831 \n    832 class RandomizedSearchCV(BaseSearchCV):\n    833     \"\"\"Randomized search on hyper parameters.\n\n...........................................................................\n/home/jeovach/anaconda3/lib/python3.5/site-packages/sklearn/grid_search.py in _fit(self=GridSearchCV(cv=5, error_score='raise',\n       e...jobs', refit=True, scoring='accuracy', verbose=1), X=array([ 'i really love this movie i haven t play...to bust itself wide open D P D P'], dtype=object), y=array([1, 1, 1, ..., 1, 1, 1]), parameter_iterable=<sklearn.grid_search.ParameterGrid object>)\n    568         )(\n    569             delayed(_fit_and_score)(clone(base_estimator), X, y, self.scorer_,\n    570                                     train, test, self.verbose, parameters,\n    571                                     self.fit_params, return_parameters=True,\n    572                                     error_score=self.error_score)\n--> 573                 for parameters in parameter_iterable\n        parameters = undefined\n        parameter_iterable = <sklearn.grid_search.ParameterGrid object>\n    574                 for train, test in cv)\n    575 \n    576         # Out is a list of triplet: score, estimator, n_test_samples\n    577         n_fits = len(out)\n\n...........................................................................\n/home/jeovach/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseSearchCV._fit.<locals>.<genexpr>>)\n    763             if pre_dispatch == \"all\" or n_jobs == 1:\n    764                 # The iterable was consumed all at once by the above for loop.\n    765                 # No need to wait for async callbacks to trigger to\n    766                 # consumption.\n    767                 self._iterating = False\n--> 768             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    769             # Make sure that we get a last message telling us we are done\n    770             elapsed_time = time.time() - self._start_time\n    771             self._print('Done %3i out of %3i | elapsed: %s finished',\n    772                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nValueError                                         Wed Mar  1 22:50:29 2017\nPID: 14881                 Python 3.5.2: /home/jeovach/anaconda3/bin/python\n...........................................................................\n/home/jeovach/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (Pipeline(steps=[('vect', TfidfVectorizer(analyze...0.0001,\n          verbose=0, warm_start=False))]), array([ 'i really love this movie i haven t play...to bust itself wide open D P D P'], dtype=object), array([1, 1, 1, ..., 1, 1, 1]), make_scorer(accuracy_score), array([ 650,  651,  652, ..., 3247, 3248, 3249]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ...40, 641, 642, 643, 644, 645, 646, 647, 648, 649]), 1, {'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer>}, {}), {'error_score': 'raise', 'return_parameters': True})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/home/jeovach/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (Pipeline(steps=[('vect', TfidfVectorizer(analyze...0.0001,\n          verbose=0, warm_start=False))]), array([ 'i really love this movie i haven t play...to bust itself wide open D P D P'], dtype=object), array([1, 1, 1, ..., 1, 1, 1]), make_scorer(accuracy_score), array([ 650,  651,  652, ..., 3247, 3248, 3249]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ...40, 641, 642, 643, 644, 645, 646, 647, 648, 649]), 1, {'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer>}, {})\n        kwargs = {'error_score': 'raise', 'return_parameters': True}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/home/jeovach/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py in _fit_and_score(estimator=Pipeline(steps=[('vect', TfidfVectorizer(analyze...0.0001,\n          verbose=0, warm_start=False))]), X=array([ 'i really love this movie i haven t play...to bust itself wide open D P D P'], dtype=object), y=array([1, 1, 1, ..., 1, 1, 1]), scorer=make_scorer(accuracy_score), train=array([ 650,  651,  652, ..., 3247, 3248, 3249]), test=array([  0,   1,   2,   3,   4,   5,   6,   7,  ...40, 641, 642, 643, 644, 645, 646, 647, 648, 649]), verbose=1, parameters={'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer>}, fit_params={}, return_train_score=False, return_parameters=True, error_score='raise')\n   1660 \n   1661     try:\n   1662         if y_train is None:\n   1663             estimator.fit(X_train, **fit_params)\n   1664         else:\n-> 1665             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method Pipeline.fit of Pipeline(steps=[('....0001,\n          verbose=0, warm_start=False))])>\n        X_train = array([ 'after a very disappointing part 3 i kin...to bust itself wide open D P D P'], dtype=object)\n        y_train = array([1, 1, 1, ..., 1, 1, 1])\n        fit_params = {}\n   1666 \n   1667     except Exception as e:\n   1668         if error_score == 'raise':\n   1669             raise\n\n...........................................................................\n/home/jeovach/anaconda3/lib/python3.5/site-packages/sklearn/pipeline.py in fit(self=Pipeline(steps=[('vect', TfidfVectorizer(analyze...0.0001,\n          verbose=0, warm_start=False))]), X=array([ 'after a very disappointing part 3 i kin...to bust itself wide open D P D P'], dtype=object), y=array([1, 1, 1, ..., 1, 1, 1]), **fit_params={})\n    265         self : Pipeline\n    266             This estimator\n    267         \"\"\"\n    268         Xt, fit_params = self._fit(X, y, **fit_params)\n    269         if self._final_estimator is not None:\n--> 270             self._final_estimator.fit(Xt, y, **fit_params)\n        self._final_estimator.fit = <bound method LogisticRegression.fit of Logistic...l=0.0001,\n          verbose=0, warm_start=False)>\n        Xt = <2600x28213 sparse matrix of type '<class 'numpy... stored elements in Compressed Sparse Row format>\n        y = array([1, 1, 1, ..., 1, 1, 1])\n        fit_params = {}\n    271         return self\n    272 \n    273     def fit_transform(self, X, y=None, **fit_params):\n    274         \"\"\"Fit the model and transform with the final estimator\n\n...........................................................................\n/home/jeovach/anaconda3/lib/python3.5/site-packages/sklearn/linear_model/logistic.py in fit(self=LogisticRegression(C=1.0, class_weight=None, dua...ol=0.0001,\n          verbose=0, warm_start=False), X=<2600x28213 sparse matrix of type '<class 'numpy... stored elements in Compressed Sparse Row format>, y=array([1, 1, 1, ..., 1, 1, 1]), sample_weight=None)\n   1181         if self.solver == 'liblinear':\n   1182             self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n   1183                 X, y, self.C, self.fit_intercept, self.intercept_scaling,\n   1184                 self.class_weight, self.penalty, self.dual, self.verbose,\n   1185                 self.max_iter, self.tol, self.random_state,\n-> 1186                 sample_weight=sample_weight)\n        sample_weight = None\n   1187             self.n_iter_ = np.array([n_iter_])\n   1188             return self\n   1189 \n   1190         if self.solver == 'sag':\n\n...........................................................................\n/home/jeovach/anaconda3/lib/python3.5/site-packages/sklearn/svm/base.py in _fit_liblinear(X=<2600x28213 sparse matrix of type '<class 'numpy... stored elements in Compressed Sparse Row format>, y=array([1, 1, 1, ..., 1, 1, 1]), C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, penalty='l1', dual=False, verbose=0, max_iter=100, tol=0.0001, random_state=0, multi_class='ovr', loss='logistic_regression', epsilon=0.1, sample_weight=None)\n    870         y_ind = enc.fit_transform(y)\n    871         classes_ = enc.classes_\n    872         if len(classes_) < 2:\n    873             raise ValueError(\"This solver needs samples of at least 2 classes\"\n    874                              \" in the data, but the data contains only one\"\n--> 875                              \" class: %r\" % classes_[0])\n        classes_ = array([1])\n    876 \n    877         class_weight_ = compute_class_weight(class_weight, classes_, y)\n    878     else:\n    879         class_weight_ = np.empty(0, dtype=np.float64)\n\nValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n___________________________________________________________________________"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                       lowercase=False,\n",
    "                       preprocessor=None)\n",
    "param_grid = [{\"vect__ngram_range\": [(1,1)],\n",
    "               \"vect__stop_words\": [stop, None],\n",
    "               \"vect__tokenizer\": [tokenizer,\n",
    "                               tokenizer_porter],\n",
    "               \"clf__penalty\": [\"l1\", \"l2\"],\n",
    "               \"clf__C\": [1.0, 10.0, 100.0]},\n",
    "              {\"vect__ngram_range\": [(1,1)],\n",
    "               \"vect__stop_words\": [stop, None],\n",
    "               \"vect__tokenizer\": [tokenizer,\n",
    "                                 tokenizer_porter],\n",
    "               \"vect__use_idf\":[False],\n",
    "               \"vect__norm\":[None],\n",
    "               \"clf__penalty\": [\"l1\", \"l2\"],\n",
    "               \"clf__C\": [1.0, 10.0, 100.0]}\n",
    "             ]\n",
    "lr_tfidf = Pipeline([(\"vect\", tfidf),\n",
    "                    (\"clf\",\n",
    "                     LogisticRegression(random_state=0))]\n",
    "                   )\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                          scoring=\"accuracy\",\n",
    "                          cv=5, verbose=1,\n",
    "                          n_jobs=-1)\n",
    "gs_lr_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(gs_lr_tfidf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'best_score_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-9accfe4c2f5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgs_lr_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'best_score_'"
     ]
    }
   ],
   "source": [
    "print(gs_lr_tfidf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'best_estimator_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-a50281e9616e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgs_lr_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'best_estimator_'"
     ]
    }
   ],
   "source": [
    "clf = gs_lr_tfidf.best_estimator_\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    text = re.sub(r\"<[^>]>\", '', text)\n",
    "    emoticons = re.findall(\"(?:|;|=)(?:-)?(?:\\)\\(|D|P)\", text)\n",
    "    text = re.sub(\"[\\W]+\", \" \", text.lower()) + \\\n",
    "            \" \".join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split()\n",
    "                if w not in stop]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def stream_docs(path):\n",
    "    with open(path, \"r\") as csv:\n",
    "        next(csv)\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"This is an excellent film, with an extraordinary cast and acting. I was very disappointed with the Academy Awards when this didn\\'t get the Oscar for best film and for best actress (Woopi Goldberg)... it certainly deserved it. In any case, take a look at it. i am sure you will enjoy it very much.\"',\n",
       " 1)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(stream_docs(\"movie_data.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minibatch(doc_stream, size):\n",
    "    docs , y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "vect = HashingVectorizer(decode_error=\"ignore\",\n",
    "                        n_features=2**21,\n",
    "                        preprocessor=None,\n",
    "                        tokenizer=tokenizer)\n",
    "print(vect)\n",
    "clf = SGDClassifier(loss=\"log\", random_state=1, n_iter=1)\n",
    "doc_stream = stream_docs(\"movie_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[##############################] | ETA: 00:00:00\n",
      "Total time elapsed: 00:01:12\n"
     ]
    }
   ],
   "source": [
    "import pyprind\n",
    "import numpy as np\n",
    "pbar = pyprind.ProgBar(45)\n",
    "classes = np.array([0,1])\n",
    "for _ in range(45):\n",
    "    X_train, y_train = get_minibatch(doc_stream,\n",
    "                                    size=1000)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87839999999999996"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test, y_test = get_minibatch(doc_stream, size=5000)\n",
    "X_test = vect.transform(X_test)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
